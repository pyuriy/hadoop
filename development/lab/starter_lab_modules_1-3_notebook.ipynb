{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Big Data Lab — Modules 1–3 Starter Notebook\n",
        "\n",
        "This notebook walks through a lightweight, local starter for:\n",
        "- Kafka topic creation, producing and consuming test messages (JSON path)\n",
        "- Optional notes for Schema Registry / Avro\n",
        "- A PySpark Structured Streaming job that reads from Kafka, performs an event-time aggregation with watermarking, and writes Parquet output with checkpointing.\n",
        "\n",
        "Assumptions:\n",
        "- You're running the lab stack from the lab docker-compose (kafka at `kafka:9092`, schema-registry at `http://schema-registry:8081`, MinIO at `minio:9000` if used).\n",
        "- This notebook runs in the `jupyter/pyspark-notebook` container and can execute PySpark and pip installs.\n",
        "\n",
        "Run cells sequentially."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "###########################\n",
        "# Install Python dependencies\n",
        "###########################\n",
        "# The container may already have some packages; install confluent-kafka for admin/producer/consumer clients.\n",
        "import sys\n",
        "import subprocess\n",
        "import pkgutil\n",
        "\n",
        "def pip_install(packages):\n",
        "    cmd = [sys.executable, '-m', 'pip', 'install', '--quiet'] + packages\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "required = []\n",
        "if not pkgutil.find_loader('confluent_kafka'):\n",
        "    required.append('confluent-kafka')\n",
        "if not pkgutil.find_loader('fastavro'):\n",
        "    required.append('fastavro')\n",
        "if required:\n",
        "    print('Installing:', required)\n",
        "    pip_install(required)\n",
        "else:\n",
        "    print('Required packages already installed.')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "###########################\n",
        "# Kafka: Admin check and topic creation\n",
        "###########################\n",
        "from confluent_kafka.admin import AdminClient, NewTopic\n",
        "import time\n",
        "\n",
        "KAFKA_BOOTSTRAP = 'kafka:9092'\n",
        "admin_conf = {'bootstrap.servers': KAFKA_BOOTSTRAP}\n",
        "admin = AdminClient(admin_conf)\n",
        "\n",
        "print('Listing existing topics (timeout=5s):')\n",
        "md = admin.list_topics(timeout=5)\n",
        "print(', '.join(sorted(list(md.topics.keys()))))\n",
        "\n",
        "topic_name = 'events'\n",
        "num_partitions = 6\n",
        "replication_factor = 1\n",
        "\n",
        "if topic_name not in md.topics:\n",
        "    print(f\"Creating topic '{topic_name}'...\")\n",
        "    new_topic = NewTopic(topic=topic_name, num_partitions=num_partitions, replication_factor=replication_factor)\n",
        "    fs = admin.create_topics([new_topic])\n",
        "    # Wait for creation\n",
        "    try:\n",
        "        fs[topic_name].result(10)\n",
        "        print('Topic created')\n",
        "    except Exception as e:\n",
        "        print('Topic creation failed or already exists:', e)\n",
        "else:\n",
        "    print(f\"Topic '{topic_name}' already exists\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "###########################\n",
        "# Produce sample JSON messages to Kafka topic\n",
        "###########################\n",
        "from confluent_kafka import Producer\n",
        "import json\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "pconf = {'bootstrap.servers': KAFKA_BOOTSTRAP}\n",
        "producer = Producer(pconf)\n",
        "def delivery_report(err, msg):\n",
        "    if err is not None:\n",
        "        print('Delivery failed:', err)\n",
        "    # else: success (we keep quiet)\n",
        "\n",
        "print('Producing 20 test messages...')\n",
        "now = datetime.utcnow()\n",
        "for i in range(20):\n",
        "    ev = {\n",
        "        'id': str(i),\n",
        "        'user_id': f'user_{random.randint(1,5)}',\n",
        "        'event_type': random.choice(['click','view','purchase']),\n",
        "        'event_time': (now - timedelta(seconds=random.randint(0,600))).isoformat() + 'Z',\n",
        "        'value': random.random()\n",
        "    }\n",
        "    producer.produce(topic_name, key=ev['user_id'], value=json.dumps(ev), callback=delivery_report)\n",
        "    # poll to trigger delivery callbacks\n",
        "    producer.poll(0)\n",
        "\n",
        "producer.flush(10)\n",
        "print('Done producing messages.')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "###########################\n",
        "# Consume messages (quick check)\n",
        "###########################\n",
        "from confluent_kafka import Consumer, KafkaException\n",
        "\n",
        "cconf = {\n",
        "    'bootstrap.servers': KAFKA_BOOTSTRAP,\n",
        "    'group.id': 'notebook-checker-' + str(random.randint(0,1000)),\n",
        "    'auto.offset.reset': 'earliest'\n",
        "}\n",
        "consumer = Consumer(cconf)\n",
        "consumer.subscribe([topic_name])\n",
        "print('Polling up to 10 messages from topic...')\n",
        "count = 0\n",
        "try:\n",
        "    while count < 10:\n",
        "        msg = consumer.poll(timeout=2.0)\n",
        "        if msg is None:\n",
        "            break\n",
        "        if msg.error():\n",
        "            raise KafkaException(msg.error())\n",
        "        print('Key:', msg.key().decode('utf-8') if msg.key() else None, 'Value:', msg.value().decode('utf-8'))\n",
        "        count += 1\n",
        "finally:\n",
        "    consumer.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PySpark Structured Streaming: read from Kafka, aggregate, write Parquet\n",
        "\n",
        "This cell creates a streaming query that:\n",
        "- Reads from Kafka (topic `events`)\n",
        "- Parses JSON payloads\n",
        "- Uses event-time (`event_time`) with watermarking\n",
        "- Aggregates counts per event_type in 5 minute tumbling windows\n        \n",
        "By default it writes Parquet files locally under `./data/stream_output` and uses `./checkpoints/events` for checkpointing.\n",
        "\n",
        "If you want to write to MinIO (S3-compatible), see the final notes cell for S3A configuration and required jars.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": [
        "###########################\n",
        "# Streaming job\n",
        "###########################\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import from_json, col, window\n",
        "from pyspark.sql.types import StructType, StringType, TimestampType, DoubleType\n",
        "import os\n",
        "\n",
        "spark = SparkSession.builder \n",
        "    .appName('notebook-streaming-example')\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define schema for JSON payload\n",
        "schema = StructType() \\\n",
        "    .add('id', StringType()) \\\n",
        "    .add('user_id', StringType()) \\\n",
        "    .add('event_type', StringType()) \\\n",
        "    .add('event_time', StringType()) \\\n",
        "    .add('value', DoubleType())\n",
        "\n",
        "kafka_df = (\n",
        "    spark.readStream.format('kafka')\n",
        "    .option('kafka.bootstrap.servers', KAFKA_BOOTSTRAP)\n",
        "    .option('subscribe', topic_name)\n",
        "    .option('startingOffsets', 'earliest')\n",
        "    .load()\n",
        ")\n",
        "# value is in bytes; cast to string\n",
        "json_df = kafka_df.select(from_json(col('value').cast('string'), schema).alias('data')).select('data.*')\n",
        "\n",
        "# Cast event_time to timestamp type; if payload already has timezone/format, adjust accordingly\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "events = json_df.withColumn('event_time', to_timestamp(col('event_time')))\n",
        "\n",
        "# Windowed aggregation: 5-minute tumbling windows with 10-minute watermark for late data\n",
        "agg = (\n",
        "    events.withWatermark('event_time', '10 minutes')\n",
        "    .groupBy(window(col('event_time'), '5 minutes'), col('event_type'))\n",
        "    .count()\n",
        ")\n",
        "\n",
        "output_path = './data/stream_output'\n",
        "checkpoint_path = './checkpoints/events'\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "os.makedirs(checkpoint_path, exist_ok=True)\n",
        "\n",
        "query = (\n",
        "    agg.writeStream\n",
        "    .format('parquet')\n",
        "    .outputMode('append')\n",
        "    .option('path', output_path)\n",
        "    .option('checkpointLocation', checkpoint_path)\n",
        "    .trigger(processingTime='10 seconds')\n",
        "    .start()\n",
        ")\n",
        "print('Streaming query started. Waiting for ~30 seconds to collect data...')\n",
        "query.awaitTermination(30)\n",
        "print('Stopping query...')\n",
        "query.stop()\n",
        "print('Query stopped.')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "###########################\n",
        "# Verify Parquet output files\n",
        "###########################\n",
        "import glob\n",
        "files = glob.glob('./data/stream_output/*/*.parquet') + glob.glob('./data/stream_output/*.parquet')\n",
        "print('Parquet files written (sample):')\n",
        "for f in files[:10]:\n",
        "    print('-', f)\n",
        "\n",
        "print('\\nYou can open the Parquet files using PySpark or pandas (pyarrow).')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes & next steps\n",
        "\n",
        "1. Schema Registry / Avro: if you want Avro-encoded messages with Schema Registry, register Avro subjects via the Schema Registry REST API and use a producer that encodes Avro. The notebook currently uses JSON for simplicity. I can add a full Avro+Schema-Registry example (produce Avro via confluent_kafka.avro.AvroProducer and parse Avro in Spark) if you'd like.\n",
        "2. Writing to MinIO (S3-compatible): to write Parquet directly to MinIO/S3 from Spark you will need the hadoop-aws and aws-java-sdk (or compatible versions) on Spark's classpath and configure spark._jsc.hadoopConfiguration with the MinIO endpoint, access key, secret key, and path-style access. Example config snippet I can add:\n",
        "   - spark._jsc.hadoopConfiguration().set('fs.s3a.endpoint','http://minio:9000')\n",
        "   - spark._jsc.hadoopConfiguration().set('fs.s3a.access.key','minioadmin')\n",
        "   - spark._jsc.hadoopConfiguration().set('fs.s3a.secret.key','minioadmin')\n",
        "   - spark._jsc.hadoopConfiguration().set('fs.s3a.path.style.access','true')\n",
        "\n",
        "3. Checkpointing: the notebook uses a local checkpoint directory. For resilience in the full lab, use a durable store (MinIO path) for checkpoints so jobs can recover across container restarts.\n",
        "\n",
        "If you want, I will now:\n",
        "- add Avro + Schema Registry cells (producer & parsing), and/or\n",
        "- add MinIO S3A configuration cells and instructions to include hadoop-aws jars for Spark, or\n",
        "- produce a spark-submit-ready Python script version of the streaming job.\n",
        "Which would you like next?"
      ]
    }
  ]
}